{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ticket-closure\n",
    "\n",
    "This notebook contains sample code to build a model which can predictt how long it will take to resolve tickets in an IT support system. It uses the [Incident Management dataset from the UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Incident+management+process+enriched+event+log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y scipy numpy pandas joblib scikit-learn\n",
    "!pip install scipy==1.4.1 numpy==1.18.1 pandas==0.24.1 joblib==0.14.1 scikit-learn==0.22.1 lightgbm==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "# https://archive.ics.uci.edu/ml/datasets/Incident+management+process+enriched+event+log\n",
    "TRAIN_FILE = \"incident_event_log.csv\"\n",
    "\n",
    "_date_parser = lambda x: pd.NaT if x == '?' else datetime.strptime(x, \"%d/%m/%Y %H:%M\")\n",
    "train_df = pd.read_csv(\n",
    "    TRAIN_FILE,\n",
    "    parse_dates=[\n",
    "        \"opened_at\",\n",
    "        \"resolved_at\",\n",
    "        \"closed_at\",\n",
    "        \"sys_created_at\",\n",
    "        \"sys_updated_at\"\n",
    "    ],\n",
    "    infer_datetime_format=False,\n",
    "    converters={\n",
    "        \"opened_at\": _date_parser,\n",
    "        \"resolved_at\": _date_parser,\n",
    "        \"closed_at\": _date_parser,\n",
    "        \"sys_created_at\": _date_parser,\n",
    "        \"sys_updated_at\": _date_parser\n",
    "    },\n",
    "    na_values = ['?']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()\n",
    "\n",
    "# drop columns\n",
    "#    * stuff you can only know after you've closed the ticket\n",
    "#    * 'number', which is an incident identifier that is super high-cardinality\n",
    "#    * high-cardinality columns that are basically obfuscated employee IDs (like 'created_by')\n",
    "#    * 'caller_id': not creating stateful features like \"number of previous tickets from this caller\"\n",
    "drop_cols = [\n",
    "    \"active\",\n",
    "    \"assigned_to\",\n",
    "    \"caller_id\",\n",
    "    \"caused_by\",\n",
    "    \"closed_code\",\n",
    "    \"incident_state\",\n",
    "    \"knowledge\",\n",
    "    \"made_sla\",\n",
    "    \"number\",\n",
    "    \"opened_by\",\n",
    "    \"resolved_at\",\n",
    "    \"resolved_by\",\n",
    "    \"rfc\",\n",
    "    \"sys_created_by\",\n",
    "    \"sys_updated_by\",\n",
    "    \"vendor\"\n",
    "]\n",
    "train_df = train_df[[col for col in train_df.columns if col not in drop_cols]]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target, 'time_to_close'\n",
    "TARGET_COL = 'time_to_close'\n",
    "train_df[TARGET_COL] = (train_df['closed_at'] - train_df['opened_at']) / np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticket_closure_lib.transformers import OrdinalConverter\n",
    "from ticket_closure_lib.transformers import DateColTransformer\n",
    "from ticket_closure_lib.transformers import FeatureRemover\n",
    "\n",
    "feature_map = {\n",
    "    \"impact\": {\n",
    "        \"3 - Low\": 1,\n",
    "        \"2 - Medium\": 2,\n",
    "        \"1 - High\": 3\n",
    "    },\n",
    "    \"priority\": {\n",
    "        \"4 - Low\": 1,\n",
    "        \"3 - Moderate\": 2,\n",
    "        \"2 - High\": 3,\n",
    "        \"1 - Critical\": 4\n",
    "    },\n",
    "    \"urgency\": {\n",
    "        \"3 - Low\": 1,\n",
    "        \"2 - Medium\": 2,\n",
    "        \"1 - High\": 3\n",
    "    }\n",
    "}\n",
    "ordinal_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('convert_some_to_int', OrdinalConverter(feature_map=feature_map)),\n",
    "        ('fill_na', SimpleImputer(strategy=\"constant\", fill_value=\"placeholder\")),\n",
    "        ('encode', OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_features = [\n",
    "    col for col in dict(train_df.dtypes).keys()\n",
    "    if (\n",
    "        train_df.dtypes[col] == np.dtype('int64') or\n",
    "        train_df.dtypes[col] == np.dtype('float64')\n",
    "    )\n",
    "]\n",
    "categorical_features = [\n",
    "    col for col in dict(train_df.dtypes).keys()\n",
    "    if train_df.dtypes[col] == np.dtype('O')\n",
    "]\n",
    "\n",
    "#  https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "# create a feature engineering Python\n",
    "lgb_estimator = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    max_depth=10,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=1000,\n",
    "    num_leaves = 30,\n",
    "    objective='regression',\n",
    "    n_jobs=4,\n",
    "    silent=False\n",
    ")\n",
    "\n",
    "cols_to_drop = [\n",
    "    \"sys_created_at\",\n",
    "    \"sys_updated_at\",\n",
    "    \"opened_at\",\n",
    "    \"closed_at\"\n",
    "]\n",
    "\n",
    "ordinal_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"placeholder\")),\n",
    "        (\"encode\", OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_cols = list(train_df.select_dtypes(\"O\").columns)\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('date_cols', DateColTransformer()),\n",
    "        ('remove_intermediate', FeatureRemover(cols_to_drop=cols_to_drop)),\n",
    "        ('convert_some_to_int', OrdinalConverter(feature_map=feature_map)),\n",
    "        ('ordinal_col_transformer', ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"ordinal\", ordinal_transformer, categorical_cols)\n",
    "            ]\n",
    "        )),\n",
    "#         ('fill_na', SimpleImputer(strategy=\"constant\", fill_value=\"placeholder\")),\n",
    "#         ('encode', OrdinalEncoder()),\n",
    "        ('regressor', lgb_estimator)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X = train_df[[col for col in train_df.columns if col is not TARGET_COL]]\n",
    "y = train_df[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = pipeline.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mod.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"median ticket duration: {round(train_df[TARGET_COL].median() / (60 * 60 * 24.0), 2)} days\")\n",
    "print(f\"MAE: {round(sklearn.metrics.mean_absolute_error(preds, y) / (60.0 * 60.0 * 24.0), 2)} days\")\n",
    "print(f\"MSE: {sklearn.metrics.mean_squared_error(preds, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "Now that the model is trained, save it to local storage so it can be used in an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mod, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Cloud Storage\n",
    "\n",
    "At this point in the notebook, we've trained a model but that model only exists on the same machine as this notebook. Let's push it to [Amazon S3](https://aws.amazon.com/s3/) so that we can pull it and re-use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "S3_TRAINING_ARTIFACT_BUCKET = \"ticket-closure-model-artifacts-358790040914-us-east-1\"\n",
    "S3 = boto3.resource('s3')\n",
    "S3.meta.client.upload_file(\n",
    "    \"model.pkl\",\n",
    "    S3_TRAINING_ARTIFACT_BUCKET,\n",
    "    \"model.pkl\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
