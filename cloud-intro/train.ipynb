{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ticket-closure\n",
    "\n",
    "This notebook contains sample code to build a model which can predictt how long it will take to resolve tickets in an IT support system. It uses the [Incident Management dataset from the UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Incident+management+process+enriched+event+log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scipy 1.4.1\n",
      "Uninstalling scipy-1.4.1:\n",
      "  Successfully uninstalled scipy-1.4.1\n",
      "Found existing installation: numpy 1.18.1\n",
      "Uninstalling numpy-1.18.1:\n",
      "  Successfully uninstalled numpy-1.18.1\n",
      "Found existing installation: pandas 0.24.1\n",
      "Uninstalling pandas-0.24.1:\n",
      "  Successfully uninstalled pandas-0.24.1\n",
      "Found existing installation: joblib 0.14.1\n",
      "Uninstalling joblib-0.14.1:\n",
      "  Successfully uninstalled joblib-0.14.1\n",
      "Found existing installation: scikit-learn 0.22.1\n",
      "Uninstalling scikit-learn-0.22.1:\n",
      "  Successfully uninstalled scikit-learn-0.22.1\n",
      "Collecting scipy==1.4.1\n",
      "  Using cached scipy-1.4.1-cp37-cp37m-macosx_10_6_intel.whl (28.4 MB)\n",
      "Collecting numpy==1.18.1\n",
      "  Using cached numpy-1.18.1-cp37-cp37m-macosx_10_9_x86_64.whl (15.1 MB)\n",
      "Collecting pandas==0.24.1\n",
      "  Using cached pandas-0.24.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (15.9 MB)\n",
      "Collecting joblib==0.14.1\n",
      "  Using cached joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "Collecting scikit-learn==0.22.1\n",
      "  Using cached scikit_learn-0.22.1-cp37-cp37m-macosx_10_6_intel.whl (11.0 MB)\n",
      "Requirement already satisfied: lightgbm==2.3.1 in /Users/jlamb/miniconda3/envs/ticket_closure/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/jlamb/miniconda3/envs/ticket_closure/lib/python3.7/site-packages (from pandas==0.24.1) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/jlamb/miniconda3/envs/ticket_closure/lib/python3.7/site-packages (from pandas==0.24.1) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jlamb/miniconda3/envs/ticket_closure/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas==0.24.1) (1.14.0)\n",
      "Installing collected packages: numpy, scipy, pandas, joblib, scikit-learn\n",
      "Successfully installed joblib-0.14.1 numpy-1.18.1 pandas-0.24.1 scikit-learn-0.22.1 scipy-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y scipy numpy pandas joblib scikit-learn\n",
    "!pip install scipy==1.4.1 numpy==1.18.1 pandas==0.24.1 joblib==0.14.1 scikit-learn==0.22.1 lightgbm==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the training data, run the provided shell script.\n",
    "\n",
    "```shell\n",
    "./scripts/get-training-data.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlamb/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (19,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "# https://archive.ics.uci.edu/ml/datasets/Incident+management+process+enriched+event+log\n",
    "TRAIN_FILE = \"incident_event_log.csv\"\n",
    "\n",
    "_date_parser = lambda x: pd.NaT if x == \"?\" else datetime.strptime(x, \"%d/%m/%Y %H:%M\")\n",
    "train_df = pd.read_csv(\n",
    "    TRAIN_FILE,\n",
    "    parse_dates=[\n",
    "        \"opened_at\",\n",
    "        \"resolved_at\",\n",
    "        \"closed_at\",\n",
    "        \"sys_created_at\",\n",
    "        \"sys_updated_at\",\n",
    "    ],\n",
    "    infer_datetime_format=False,\n",
    "    converters={\n",
    "        \"opened_at\": _date_parser,\n",
    "        \"resolved_at\": _date_parser,\n",
    "        \"closed_at\": _date_parser,\n",
    "        \"sys_created_at\": _date_parser,\n",
    "        \"sys_updated_at\": _date_parser,\n",
    "    },\n",
    "    na_values=[\"?\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reassignment_count</th>\n",
       "      <th>reopen_count</th>\n",
       "      <th>sys_mod_count</th>\n",
       "      <th>opened_at</th>\n",
       "      <th>sys_created_at</th>\n",
       "      <th>sys_updated_at</th>\n",
       "      <th>contact_type</th>\n",
       "      <th>location</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>u_symptom</th>\n",
       "      <th>cmdb_ci</th>\n",
       "      <th>impact</th>\n",
       "      <th>urgency</th>\n",
       "      <th>priority</th>\n",
       "      <th>assignment_group</th>\n",
       "      <th>u_priority_confirmation</th>\n",
       "      <th>notify</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>closed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02-29 01:16:00</td>\n",
       "      <td>2016-02-29 01:23:00</td>\n",
       "      <td>2016-02-29 01:23:00</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Location 143</td>\n",
       "      <td>Category 55</td>\n",
       "      <td>Subcategory 170</td>\n",
       "      <td>Symptom 72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>3 - Moderate</td>\n",
       "      <td>Group 56</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-05 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-02-29 01:16:00</td>\n",
       "      <td>2016-02-29 01:23:00</td>\n",
       "      <td>2016-02-29 08:53:00</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Location 143</td>\n",
       "      <td>Category 55</td>\n",
       "      <td>Subcategory 170</td>\n",
       "      <td>Symptom 72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>3 - Moderate</td>\n",
       "      <td>Group 56</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-05 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2016-02-29 01:16:00</td>\n",
       "      <td>2016-02-29 01:23:00</td>\n",
       "      <td>2016-02-29 11:29:00</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Location 143</td>\n",
       "      <td>Category 55</td>\n",
       "      <td>Subcategory 170</td>\n",
       "      <td>Symptom 72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>3 - Moderate</td>\n",
       "      <td>Group 56</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-05 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-02-29 01:16:00</td>\n",
       "      <td>2016-02-29 01:23:00</td>\n",
       "      <td>2016-03-05 12:00:00</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Location 143</td>\n",
       "      <td>Category 55</td>\n",
       "      <td>Subcategory 170</td>\n",
       "      <td>Symptom 72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>3 - Moderate</td>\n",
       "      <td>Group 56</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-05 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02-29 04:40:00</td>\n",
       "      <td>2016-02-29 04:57:00</td>\n",
       "      <td>2016-02-29 04:57:00</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Location 165</td>\n",
       "      <td>Category 40</td>\n",
       "      <td>Subcategory 215</td>\n",
       "      <td>Symptom 471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>2 - Medium</td>\n",
       "      <td>3 - Moderate</td>\n",
       "      <td>Group 70</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-06 10:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reassignment_count  reopen_count  sys_mod_count           opened_at  \\\n",
       "0                   0             0              0 2016-02-29 01:16:00   \n",
       "1                   0             0              2 2016-02-29 01:16:00   \n",
       "2                   0             0              3 2016-02-29 01:16:00   \n",
       "3                   0             0              4 2016-02-29 01:16:00   \n",
       "4                   0             0              0 2016-02-29 04:40:00   \n",
       "\n",
       "       sys_created_at      sys_updated_at contact_type      location  \\\n",
       "0 2016-02-29 01:23:00 2016-02-29 01:23:00        Phone  Location 143   \n",
       "1 2016-02-29 01:23:00 2016-02-29 08:53:00        Phone  Location 143   \n",
       "2 2016-02-29 01:23:00 2016-02-29 11:29:00        Phone  Location 143   \n",
       "3 2016-02-29 01:23:00 2016-03-05 12:00:00        Phone  Location 143   \n",
       "4 2016-02-29 04:57:00 2016-02-29 04:57:00        Phone  Location 165   \n",
       "\n",
       "      category      subcategory    u_symptom cmdb_ci      impact     urgency  \\\n",
       "0  Category 55  Subcategory 170   Symptom 72     NaN  2 - Medium  2 - Medium   \n",
       "1  Category 55  Subcategory 170   Symptom 72     NaN  2 - Medium  2 - Medium   \n",
       "2  Category 55  Subcategory 170   Symptom 72     NaN  2 - Medium  2 - Medium   \n",
       "3  Category 55  Subcategory 170   Symptom 72     NaN  2 - Medium  2 - Medium   \n",
       "4  Category 40  Subcategory 215  Symptom 471     NaN  2 - Medium  2 - Medium   \n",
       "\n",
       "       priority assignment_group  u_priority_confirmation         notify  \\\n",
       "0  3 - Moderate         Group 56                    False  Do Not Notify   \n",
       "1  3 - Moderate         Group 56                    False  Do Not Notify   \n",
       "2  3 - Moderate         Group 56                    False  Do Not Notify   \n",
       "3  3 - Moderate         Group 56                    False  Do Not Notify   \n",
       "4  3 - Moderate         Group 70                    False  Do Not Notify   \n",
       "\n",
       "  problem_id           closed_at  \n",
       "0        NaN 2016-03-05 12:00:00  \n",
       "1        NaN 2016-03-05 12:00:00  \n",
       "2        NaN 2016-03-05 12:00:00  \n",
       "3        NaN 2016-03-05 12:00:00  \n",
       "4        NaN 2016-03-06 10:00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n",
    "\n",
    "# drop columns\n",
    "#    * stuff you can only know after you've closed the ticket\n",
    "#    * 'number', which is an incident identifier that is super high-cardinality\n",
    "#    * high-cardinality columns that are basically obfuscated employee IDs (like 'created_by')\n",
    "#    * 'caller_id': not creating stateful features like \"number of previous tickets from this caller\"\n",
    "drop_cols = [\n",
    "    \"active\",\n",
    "    \"assigned_to\",\n",
    "    \"caller_id\",\n",
    "    \"caused_by\",\n",
    "    \"closed_code\",\n",
    "    \"incident_state\",\n",
    "    \"knowledge\",\n",
    "    \"made_sla\",\n",
    "    \"number\",\n",
    "    \"opened_by\",\n",
    "    \"resolved_at\",\n",
    "    \"resolved_by\",\n",
    "    \"rfc\",\n",
    "    \"sys_created_by\",\n",
    "    \"sys_updated_by\",\n",
    "    \"vendor\",\n",
    "]\n",
    "train_df = train_df[[col for col in train_df.columns if col not in drop_cols]]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target, 'time_to_close'\n",
    "TARGET_COL = \"time_to_close\"\n",
    "train_df[TARGET_COL] = (\n",
    "    train_df[\"closed_at\"] - train_df[\"sys_updated_at\"]\n",
    ") / np.timedelta64(1, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticket_closure_lib.transformers import OrdinalConverter\n",
    "from ticket_closure_lib.transformers import DateColTransformer\n",
    "from ticket_closure_lib.transformers import FeatureRemover\n",
    "\n",
    "feature_map = {\n",
    "    \"impact\": {\"3 - Low\": 1, \"2 - Medium\": 2, \"1 - High\": 3},\n",
    "    \"priority\": {\"4 - Low\": 1, \"3 - Moderate\": 2, \"2 - High\": 3, \"1 - Critical\": 4},\n",
    "    \"urgency\": {\"3 - Low\": 1, \"2 - Medium\": 2, \"1 - High\": 3},\n",
    "}\n",
    "ordinal_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"convert_some_to_int\", OrdinalConverter(feature_map=feature_map)),\n",
    "        (\"fill_na\", SimpleImputer(strategy=\"constant\", fill_value=\"placeholder\")),\n",
    "        (\"encode\", OrdinalEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_features = [\n",
    "    col\n",
    "    for col in dict(train_df.dtypes).keys()\n",
    "    if (\n",
    "        train_df.dtypes[col] == np.dtype(\"int64\")\n",
    "        or train_df.dtypes[col] == np.dtype(\"float64\")\n",
    "    )\n",
    "]\n",
    "categorical_features = [\n",
    "    col for col in dict(train_df.dtypes).keys() if train_df.dtypes[col] == np.dtype(\"O\")\n",
    "]\n",
    "\n",
    "#  https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "# create a feature engineering Python\n",
    "lgb_estimator = lgb.LGBMRegressor(\n",
    "    boosting_type=\"gbdt\",\n",
    "    max_depth=10,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=1000,\n",
    "    num_leaves=30,\n",
    "    objective=\"regression\",\n",
    "    n_jobs=4,\n",
    "    silent=False,\n",
    ")\n",
    "\n",
    "cols_to_drop = [\"sys_created_at\", \"sys_updated_at\", \"opened_at\", \"closed_at\"]\n",
    "\n",
    "ordinal_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"placeholder\")),\n",
    "        (\"encode\", OrdinalEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_cols = list(train_df.select_dtypes(\"O\").columns)\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"date_cols\", DateColTransformer()),\n",
    "        (\"remove_intermediate\", FeatureRemover(cols_to_drop=cols_to_drop)),\n",
    "        (\"convert_some_to_int\", OrdinalConverter(feature_map=feature_map)),\n",
    "        (\n",
    "            \"ordinal_col_transformer\",\n",
    "            ColumnTransformer(\n",
    "                transformers=[(\"ordinal\", ordinal_transformer, categorical_cols)]\n",
    "            ),\n",
    "        ),\n",
    "        #         ('fill_na', SimpleImputer(strategy=\"constant\", fill_value=\"placeholder\")),\n",
    "        #         ('encode', OrdinalEncoder()),\n",
    "        (\"regressor\", lgb_estimator),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X = train_df[[col for col in train_df.columns if col is not TARGET_COL]]\n",
    "y = train_df[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = pipeline.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mod.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median ticket duration: 5.22 days\n",
      "MAE: 7.62 days\n",
      "MSE: 2080208010448.1216\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"median ticket duration: {round(train_df[TARGET_COL].median() / (60 * 60 * 24.0), 2)} days\"\n",
    ")\n",
    "print(\n",
    "    f\"MAE: {round(sklearn.metrics.mean_absolute_error(preds, y) / (60.0 * 60.0 * 24.0), 2)} days\"\n",
    ")\n",
    "print(f\"MSE: {sklearn.metrics.mean_squared_error(preds, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "Now that the model is trained, save it to local storage so it can be used in an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e1ea7ef909a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mLOCAL_MODEL_FILE_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOCAL_MODEL_FILE_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "LOCAL_MODEL_FILE_NAME = \"model.pkl\"\n",
    "with open(LOCAL_MODEL_FILE_NAME, \"wb\") as f:\n",
    "    pickle.dump(mod, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Cloud Storage\n",
    "\n",
    "At this point in the notebook, we've trained a model but that model only exists on the same machine as this notebook. Let's push it to [Amazon S3](https://aws.amazon.com/s3/) so that we can pull it and re-use it later.\n",
    "\n",
    "`S3_TRAINING_ARTIFACT_BUCKET` below should be replaced with the name of the bucket you created in CloudFormation. Visit the AWS CloudFormation section of the console to find that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "S3_TRAINING_ARTIFACT_BUCKET = \"ticket-closure-model-artifacts-358790040914-us-east-1\"\n",
    "S3 = boto3.resource(\"s3\")\n",
    "S3.meta.client.upload_file(\n",
    "    LOCAL_MODEL_FILE_NAME, S3_TRAINING_ARTIFACT_BUCKET, LOCAL_MODEL_FILE_NAME\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
