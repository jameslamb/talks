{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Machine Learning with Python and Dask\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook uses [`dask-cloudprovider`](https://github.com/dask/dask-cloudprovider) to create a Dask cluster in Amazon ECS, a container orchestration service from AWS.\n",
    "\n",
    "This noteboook assumes that you've already configured access to AWS. See [the AWS docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#configuration) for details on how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install blosc==1.9.2 lz4==3.1.0 dask==2.27.0 dask-ml==1.6.0 numpy==1.18.1 scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import warnings\n",
    "\n",
    "from dask_cloudprovider import FargateCluster\n",
    "from dask.distributed import Client, wait\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from dask_ml.compose import ColumnTransformer\n",
    "from dask_ml.preprocessing import StandardScaler, DummyEncoder, Categorizer\n",
    "from dask_ml.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [\n",
    "    'pickup_weekday',\n",
    "    'pickup_weekofyear',\n",
    "    'pickup_hour',\n",
    "    'pickup_week_hour',\n",
    "    'pickup_minute',\n",
    "    'passenger_count',\n",
    "]\n",
    "categorical_feat = [\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = 'tip_fraction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 6\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-2\"\n",
    "\n",
    "cluster = FargateCluster(\n",
    "    image=\"daskdev/dask:latest\",\n",
    "    worker_mem=30720,\n",
    "    n_workers=n_workers,\n",
    "    fargate_use_private_ip=False,\n",
    "    scheduler_timeout=\"45 minutes\",\n",
    "    environment={\n",
    "        \"EXTRA_PIP_PACKAGES\": \"dask-ml==1.6.0 scikit-learn==0.23.2 s3fs\"\n",
    "    }\n",
    ")\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dashboard (link above ^) and watch it when you execute some commands, you'll see which tasks are running across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scheduler_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I changed my mind...let's add more workers! `FargateCluster` sub-classes `SpecCluster` from `dask.distributed`. That class allows you to programmatically scale a cluster up and down.\n",
    "\n",
    "Run the line below, then visit the ECS console in AWS. You should see two more workers spin up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(n_workers + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and feature engineering\n",
    "\n",
    "The code below creates a Dask Dataframe from a collection of CSV files in S3. Doesn't it look like `pandas` code?\n",
    "\n",
    "After this cell, no data has actually been pulled. The DataFrame is just a task graph at this point, and won't be computed until we ask it for something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "csvs = fs.ls('s3://nyc-tlc/trip data/')\n",
    "csvs = [\n",
    "    f\"s3://{x}\" for x in csvs\n",
    "    if 'yellow' in x and ('2018' in x)\n",
    "]\n",
    "\n",
    "cols = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']\n",
    "\n",
    "ddf = dd.read_csv(\n",
    "    csvs,\n",
    "    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "    storage_options={'anon': True},\n",
    "    assume_missing=True,\n",
    "    usecols=cols\n",
    ").sample(frac=0.01, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the Dask dashboard before running the line below. Then you can watch the individual tasks that have to contribute to get an answer to this\n",
    "\n",
    "> how many rows are there in ddf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Num rows: {len(ddf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size: {ddf.memory_usage(deep=True).sum().compute() / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask Dataframes as lazily evaluated. Right now, that dataframe is just a collection of function calls waiting to be executed.\n",
    "\n",
    "To materialize it (so we don't have to keep re-reading the data from S3), we can use `persist()`. This tells the scheduler to tell workers to execute all the tasks needed to read in the data. `persist()` is asynchronous...it won't wait for all of those tasks to complete before returning.\n",
    "\n",
    "Using `wait()` allows us to say \"wait until the datta frame has been completely read\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = ddf.persist()\n",
    "_ = wait(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate features from a raw taxi dataframe.\n",
    "    \"\"\"\n",
    "    df = df[df.fare_amount > 0]  # avoid divide-by-zero\n",
    "    df['tip_fraction'] = df.tip_amount / df.fare_amount\n",
    "    \n",
    "    df['pickup_weekday'] = df.tpep_pickup_datetime.dt.weekday\n",
    "    df['pickup_weekofyear'] = df.tpep_pickup_datetime.dt.weekofyear\n",
    "    df['pickup_hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "    df['pickup_week_hour'] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df['pickup_minute'] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [y_col]].astype(float).fillna(-1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "taxi_train = prep_df(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the preprocessing and `GridSearchCV` classes from dask-ml, but still use the scikit-learn `RandomForestRegressor` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('categorize', Categorizer(columns=categorical_feat)),\n",
    "    ('onehot', DummyEncoder(columns=categorical_feat)),\n",
    "    ('scale', ColumnTransformer(transformers=[('num', StandardScaler(), numeric_feat)])),\n",
    "    ('reg', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'reg__n_estimators': [50, 100],\n",
    "    'reg__max_depth': [3, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    params,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the Dask dashboard after you run the cell below, you'll see the grid search in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = grid_search.fit(taxi_train[features], taxi_train[y_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the MSE from the best model, and compare that to the summary statistics from the target to see how closely we've fit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train[y_col].describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "The fitted pipeline object includes a `best_estimator_`, a serializable model fit with the set of hyperparameters that had the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "\n",
    "with open(\"tip-predictor.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://docs.dask.org/en/latest/setup/docker.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
